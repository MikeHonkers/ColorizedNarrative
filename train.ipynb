{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c054334-585c-46ba-97a7-adc8152c11d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Whisper-small/base + PEFT/LoRA на SOVA-audiobooks-100k\n",
    "- Без torchcodec: Audio(decode=False) + свой коллатор (soundfile/librosa)\n",
    "- 30s crop, приводим к (80, 3000) — как требует Whisper\n",
    "- Патч PEFT forward: не передавать input_ids в Whisper (только input_features)\n",
    "\"\"\"\n",
    "\n",
    "import os, re, io, json, random\n",
    "from typing import List, Dict\n",
    "\n",
    "BASE_DIR   = r\"W:\\whisper_sova\"\n",
    "MODEL_DIR  = fr\"{BASE_DIR}\\model\"\n",
    "DATA_DIR   = fr\"{BASE_DIR}\\data\"\n",
    "OUTPUT_DIR = fr\"{BASE_DIR}\\output\"\n",
    "CACHE_DIR  = fr\"{BASE_DIR}\\hf_cache\"\n",
    "\n",
    "for p in [BASE_DIR, MODEL_DIR, DATA_DIR, OUTPUT_DIR, CACHE_DIR]:\n",
    "    os.makedirs(p, exist_ok=True)\n",
    "\n",
    "os.environ[\"HF_HOME\"] = CACHE_DIR\n",
    "os.environ[\"HF_DATASETS_CACHE\"] = fr\"{CACHE_DIR}\\datasets\"\n",
    "os.environ[\"TRANSFORMERS_CACHE\"] = MODEL_DIR\n",
    "os.environ[\"DATASETS_DISABLE_MP\"] = \"1\"\n",
    "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\"\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "import soundfile as sf\n",
    "import librosa\n",
    "import evaluate\n",
    "import requests\n",
    "\n",
    "from datasets import load_dataset, Audio, config as ds_config\n",
    "from transformers import (\n",
    "    WhisperProcessor,\n",
    "    WhisperForConditionalGeneration,\n",
    "    Trainer,\n",
    "    TrainerCallback,\n",
    ")\n",
    "from transformers.training_args import TrainingArguments\n",
    "from transformers.models.whisper.configuration_whisper import WhisperConfig\n",
    "\n",
    "from peft import LoraConfig, get_peft_model\n",
    "import peft.peft_model as peft_model_mod\n",
    "\n",
    "ds_config.TORCHCODEC_AVAILABLE = False\n",
    "\n",
    "torch.backends.cudnn.benchmark = True\n",
    "torch.set_float32_matmul_precision(\"high\")\n",
    "\n",
    "print(\"torch:\", torch.__version__)\n",
    "print(\"CUDA available:\", torch.cuda.is_available())\n",
    "if torch.cuda.is_available():\n",
    "    print(\"GPU:\", torch.cuda.get_device_name(0))\n",
    "use_bf16 = hasattr(torch.cuda, \"is_bf16_supported\") and torch.cuda.is_bf16_supported()\n",
    "print(\"bf16 supported:\", use_bf16)\n",
    "\n",
    "print(\"→ Загружаем SOVA-audiobooks-100k ...\")\n",
    "DATASET_ID = \"MikeHonkers/SOVA-audiobooks-100k\"\n",
    "ds = load_dataset(DATASET_ID, cache_dir=DATA_DIR, split=\"train\")\n",
    "\n",
    "ds = ds.train_test_split(test_size=0.02, seed=42)\n",
    "train_ds, test_holdout = ds[\"train\"], ds[\"test\"]\n",
    "val_test = test_holdout.train_test_split(test_size=0.5, seed=42)\n",
    "val_ds, test_ds = val_test[\"train\"], val_test[\"test\"]\n",
    "\n",
    "train_ds = train_ds.cast_column(\"audio\", Audio(decode=False))\n",
    "val_ds   = val_ds.cast_column(\"audio\",   Audio(decode=False))\n",
    "test_ds  = test_ds.cast_column(\"audio\",  Audio(decode=False))\n",
    "\n",
    "def normalize_ru_text(s: str) -> str:\n",
    "    s = s.strip().replace(\"ё\", \"е\")\n",
    "    s = re.sub(r'[“”«»]', '\"', s)\n",
    "    s = re.sub(r\"\\s+\", \" \", s)\n",
    "    return s\n",
    "\n",
    "def prepare_batched(batch):\n",
    "    return {\"sentence\": [normalize_ru_text(t) for t in batch[\"text\"]]}\n",
    "\n",
    "print(\"→ Нормализуем текст ...\")\n",
    "train_ds = train_ds.map(prepare_batched, batched=True, desc=\"prepare train\")\n",
    "val_ds   = val_ds.map(prepare_batched,   batched=True, desc=\"prepare val\")\n",
    "test_ds  = test_ds.map(prepare_batched,  batched=True, desc=\"prepare test\")\n",
    "\n",
    "def has_audio_ref(ex):\n",
    "    a = ex.get(\"audio\", {})\n",
    "    return (a.get(\"path\") or a.get(\"bytes\")) is not None\n",
    "\n",
    "train_ds = train_ds.filter(has_audio_ref)\n",
    "val_ds   = val_ds.filter(has_audio_ref)\n",
    "test_ds  = test_ds.filter(has_audio_ref)\n",
    "\n",
    "TARGET_SR = 16000\n",
    "MIN_BYTES = int(0.20 * TARGET_SR * 2)\n",
    "\n",
    "def looks_nonempty(ex):\n",
    "    a = ex.get(\"audio\", {})\n",
    "    if a.get(\"bytes\"):\n",
    "        return len(a[\"bytes\"]) >= MIN_BYTES\n",
    "    p = a.get(\"path\")\n",
    "    return bool(p) and os.path.exists(p) and os.path.getsize(p) >= MIN_BYTES\n",
    "\n",
    "train_ds = train_ds.filter(looks_nonempty)\n",
    "val_ds   = val_ds.filter(looks_nonempty)\n",
    "test_ds  = test_ds.filter(looks_nonempty)\n",
    "\n",
    "print(f\"train: {len(train_ds)}  val: {len(val_ds)}  test: {len(test_ds)}\")\n",
    "\n",
    "MODEL_ID = \"openai/whisper-small\"\n",
    "\n",
    "print(\"→ Загружаем процессор/модель ...\")\n",
    "processor = WhisperProcessor.from_pretrained(\n",
    "    MODEL_ID, language=\"russian\", task=\"transcribe\", cache_dir=MODEL_DIR\n",
    ")\n",
    "model = WhisperForConditionalGeneration.from_pretrained(\n",
    "    MODEL_ID, cache_dir=MODEL_DIR, device_map=\"auto\"\n",
    ")\n",
    "\n",
    "model.config.forced_decoder_ids = processor.get_decoder_prompt_ids(\n",
    "    language=\"russian\", task=\"transcribe\"\n",
    ")\n",
    "\n",
    "base_cfg = WhisperConfig.from_pretrained(MODEL_ID, cache_dir=MODEL_DIR)\n",
    "model.config.suppress_tokens = base_cfg.suppress_tokens\n",
    "model.config.begin_suppress_tokens = base_cfg.begin_suppress_tokens\n",
    "\n",
    "model.config.use_cache = False\n",
    "\n",
    "lora_cfg = LoraConfig(\n",
    "    r=8,\n",
    "    lora_alpha=16,\n",
    "    target_modules=[\"q_proj\", \"v_proj\"],\n",
    "    lora_dropout=0.05,\n",
    "    bias=\"none\",\n",
    "    task_type=\"SEQ_2_SEQ_LM\",\n",
    ")\n",
    "model = get_peft_model(model, lora_cfg)\n",
    "model.print_trainable_parameters()\n",
    "if torch.cuda.is_available():\n",
    "    model.to(\"cuda\")\n",
    "\n",
    "def _whisper_safe_forward(\n",
    "    self,\n",
    "    input_ids=None,\n",
    "    attention_mask=None,\n",
    "    inputs_embeds=None,\n",
    "    decoder_input_ids=None,\n",
    "    decoder_attention_mask=None,\n",
    "    decoder_inputs_embeds=None,\n",
    "    labels=None,\n",
    "    output_attentions=None,\n",
    "    output_hidden_states=None,\n",
    "    return_dict=None,\n",
    "    task_ids=None,\n",
    "    **kwargs,\n",
    "):\n",
    "    base_kwargs = {}\n",
    "    if \"input_features\" in kwargs and kwargs[\"input_features\"] is not None:\n",
    "        base_kwargs[\"input_features\"] = kwargs[\"input_features\"]\n",
    "    if attention_mask is not None:\n",
    "        base_kwargs[\"attention_mask\"] = attention_mask\n",
    "    if decoder_input_ids is not None:\n",
    "        base_kwargs[\"decoder_input_ids\"] = decoder_input_ids\n",
    "    if decoder_attention_mask is not None:\n",
    "        base_kwargs[\"decoder_attention_mask\"] = decoder_attention_mask\n",
    "    if decoder_inputs_embeds is not None:\n",
    "        base_kwargs[\"decoder_inputs_embeds\"] = decoder_inputs_embeds\n",
    "    if labels is not None:\n",
    "        base_kwargs[\"labels\"] = labels\n",
    "    if output_attentions is not None:\n",
    "        base_kwargs[\"output_attentions\"] = output_attentions\n",
    "    if output_hidden_states is not None:\n",
    "        base_kwargs[\"output_hidden_states\"] = output_hidden_states\n",
    "    if return_dict is not None:\n",
    "        base_kwargs[\"return_dict\"] = return_dict\n",
    "    for k in (\n",
    "        \"use_cache\",\n",
    "        \"head_mask\",\n",
    "        \"decoder_head_mask\",\n",
    "        \"cross_attn_head_mask\",\n",
    "        \"encoder_outputs\",\n",
    "        \"decoder_position_ids\",\n",
    "        \"past_key_values\",\n",
    "        \"cache_position\",\n",
    "    ):\n",
    "        if k in kwargs and kwargs[k] is not None:\n",
    "            base_kwargs[k] = kwargs[k]\n",
    "\n",
    "    return self.model(**base_kwargs)\n",
    "\n",
    "peft_model_mod.PeftModelForSeq2SeqLM.forward = _whisper_safe_forward\n",
    "\n",
    "random.seed(42)\n",
    "np.random.seed(42)\n",
    "\n",
    "wer_metric = evaluate.load(\"wer\")\n",
    "\n",
    "MAX_FRAMES = 3000\n",
    "WIN_SAMPLES = int(30.0 * TARGET_SR)\n",
    "\n",
    "def crop_30s(y: np.ndarray) -> np.ndarray:\n",
    "    if len(y) > WIN_SAMPLES:\n",
    "        start = np.random.randint(0, len(y) - WIN_SAMPLES + 1)\n",
    "        return y[start:start+WIN_SAMPLES]\n",
    "    return y\n",
    "\n",
    "def _pad_trunc_features(feats: torch.Tensor) -> torch.Tensor:\n",
    "    B, C, T = feats.shape\n",
    "    if T == MAX_FRAMES:\n",
    "        return feats\n",
    "    if T > MAX_FRAMES:\n",
    "        return feats[:, :, :MAX_FRAMES]\n",
    "    pad = torch.zeros((B, C, MAX_FRAMES - T), dtype=feats.dtype, device=feats.device)\n",
    "    return torch.cat([feats, pad], dim=-1)\n",
    "\n",
    "def load_wav_entry(audio_field: dict) -> np.ndarray:\n",
    "    if audio_field.get(\"bytes\"):\n",
    "        y, sr = sf.read(io.BytesIO(audio_field[\"bytes\"]), always_2d=False)\n",
    "    else:\n",
    "        p = audio_field.get(\"path\")\n",
    "        if not p:\n",
    "            raise FileNotFoundError(\"no audio bytes/path\")\n",
    "        if isinstance(p, str) and p.startswith((\"http://\", \"https://\")):\n",
    "            r = requests.get(p, timeout=60)\n",
    "            r.raise_for_status()\n",
    "            y, sr = sf.read(io.BytesIO(r.content), always_2d=False)\n",
    "        else:\n",
    "            y, sr = sf.read(p, always_2d=False)\n",
    "    if y.ndim > 1:\n",
    "        y = np.mean(y, axis=1)\n",
    "    y = np.nan_to_num(y, nan=0.0, posinf=0.0, neginf=0.0).astype(np.float32)\n",
    "    if sr != TARGET_SR:\n",
    "        y = librosa.resample(y, orig_sr=sr, target_sr=TARGET_SR).astype(np.float32)\n",
    "    if y.size == 0:\n",
    "        y = np.zeros(int(0.1 * TARGET_SR), dtype=np.float32)\n",
    "    return y\n",
    "\n",
    "def data_collator(batch):\n",
    "    audios = [crop_30s(load_wav_entry(b[\"audio\"])) for b in batch]\n",
    "\n",
    "    feats = processor.feature_extractor(\n",
    "        audios,\n",
    "        sampling_rate=TARGET_SR,\n",
    "        return_tensors=\"pt\",\n",
    "        padding=\"longest\",\n",
    "        truncation=False,\n",
    "        return_attention_mask=False,\n",
    "    )\n",
    "    input_features = feats.input_features\n",
    "\n",
    "    input_features = _pad_trunc_features(input_features)\n",
    "\n",
    "    labs = processor.tokenizer(\n",
    "        [b[\"sentence\"] for b in batch],\n",
    "        return_tensors=\"pt\",\n",
    "        padding=True,\n",
    "    )\n",
    "    labels = labs.input_ids\n",
    "    labels[labels == processor.tokenizer.pad_token_id] = -100\n",
    "\n",
    "    return {\"input_features\": input_features, \"labels\": labels}\n",
    "\n",
    "def compute_metrics(pred):\n",
    "    pred_ids = pred.predictions[0] if isinstance(pred.predictions, tuple) else pred.predictions\n",
    "    pred_ids = np.argmax(pred_ids, axis=-1)\n",
    "    pred_str = processor.batch_decode(pred_ids, skip_special_tokens=True)\n",
    "\n",
    "    label_ids = pred.label_ids.copy()\n",
    "    label_ids[label_ids == -100] = processor.tokenizer.pad_token_id\n",
    "    label_str = processor.tokenizer.batch_decode(label_ids, skip_special_tokens=True)\n",
    "    return {\"wer\": wer_metric.compute(references=label_str, predictions=pred_str)}\n",
    "\n",
    "batch0 = data_collator([train_ds[i] for i in range(min(4, len(train_ds)))])\n",
    "print(\"→ Collator OK:\", tuple(batch0[\"input_features\"].shape))  # (B, 80, 3000)\n",
    "for k, v in batch0.items():\n",
    "    if isinstance(v, torch.Tensor):\n",
    "        batch0[k] = v.to(model.device)\n",
    "with torch.amp.autocast(\"cuda\", enabled=torch.cuda.is_available() and not use_bf16):\n",
    "    out = model(**batch0)\n",
    "print(\"Sanity loss:\", float(out.loss.detach()))\n",
    "\n",
    "class NoEvalCallback(TrainerCallback):\n",
    "    def on_step_end(self, args, state, control, **kw): return control\n",
    "\n",
    "args = TrainingArguments(\n",
    "    output_dir=OUTPUT_DIR,\n",
    "    per_device_train_batch_size=6,\n",
    "    gradient_accumulation_steps=2,\n",
    "    learning_rate=3e-4,\n",
    "    warmup_steps=300,\n",
    "    max_steps=8000,\n",
    "    logging_steps=200,\n",
    "    fp16=not use_bf16,\n",
    "    bf16=use_bf16,\n",
    "    gradient_checkpointing=False,\n",
    "    dataloader_num_workers=0,\n",
    "    report_to=\"none\",\n",
    "    remove_unused_columns=False,\n",
    "    save_strategy=\"steps\",\n",
    "    save_steps=4000,\n",
    ")\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=args,\n",
    "    data_collator=data_collator,\n",
    "    train_dataset=train_ds,\n",
    "    eval_dataset=val_ds,\n",
    "    compute_metrics=compute_metrics,\n",
    "    callbacks=[NoEvalCallback()],\n",
    ")\n",
    "trainer.args.label_names = [\"labels\"]\n",
    "\n",
    "def _noreset_save(self, *a, **kw):\n",
    "    print(\"Skip checkpoint body to avoid dataloader reset\")\n",
    "trainer._save_checkpoint = _noreset_save.__get__(trainer, Trainer)\n",
    "\n",
    "print(\"→ Trainable %:\",\n",
    "      100 * sum(p.numel() for p in model.parameters() if p.requires_grad) /\n",
    "      sum(p.numel() for p in model.parameters()))\n",
    "print(\"→ Начинаем обучение LoRA ...\")\n",
    "trainer.train()\n",
    "\n",
    "adapter_dir   = fr\"{OUTPUT_DIR}\\lora_adapter_fast\"\n",
    "processor_dir = fr\"{OUTPUT_DIR}\\processor\"\n",
    "os.makedirs(adapter_dir, exist_ok=True)\n",
    "model.save_pretrained(adapter_dir)\n",
    "processor.save_pretrained(processor_dir)\n",
    "print(f\"LoRA адаптер сохранён: {adapter_dir}\")\n",
    "print(f\"Processor сохранён:      {processor_dir}\")\n",
    "\n",
    "print(\"→ Sanity-инференс с безопасными настройками...\")\n",
    "\n",
    "from transformers import pipeline\n",
    "from peft import PeftModel\n",
    "\n",
    "base_model = WhisperForConditionalGeneration.from_pretrained(MODEL_ID, cache_dir=MODEL_DIR, device_map=\"auto\")\n",
    "ft_model  = PeftModel.from_pretrained(base_model, adapter_dir)\n",
    "ft_model.eval()\n",
    "\n",
    "pipe = pipeline(\n",
    "    task=\"automatic-speech-recognition\",\n",
    "    model=ft_model,\n",
    "    tokenizer=processor.tokenizer,\n",
    "    feature_extractor=processor.feature_extractor,\n",
    "    generate_kwargs={\n",
    "        \"task\": \"transcribe\",\n",
    "        \"language\": \"russian\",\n",
    "        \"max_new_tokens\": 64,\n",
    "        \"no_repeat_ngram_size\": 3,\n",
    "        \"repetition_penalty\": 1.1,\n",
    "        \"length_penalty\": 1.0,\n",
    "        \"temperature\": 0.8,\n",
    "        \"return_timestamps\": False,\n",
    "        \"num_beams\": 5,\n",
    "        \"do_sample\": False,\n",
    "    },\n",
    ")\n",
    "\n",
    "if len(val_ds):\n",
    "    sample = val_ds[0][\"audio\"]\n",
    "    if sample.get(\"bytes\"):\n",
    "        tmp_path = fr\"{OUTPUT_DIR}\\_tmp.wav\"\n",
    "        with open(tmp_path, \"wb\") as f:\n",
    "            f.write(sample[\"bytes\"])\n",
    "        test_path = tmp_path\n",
    "    else:\n",
    "        test_path = sample[\"path\"]\n",
    "    res = pipe(test_path)\n",
    "    print(\"ASR sample:\", json.dumps(res, ensure_ascii=False)[:400], \"...\")\n",
    "else:\n",
    "    print(\"val_ds пуст — пропускаю sanity-инференс.\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
